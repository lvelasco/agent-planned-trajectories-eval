{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of planned agent trajectories\n",
    "\n",
    "- This Jupyter notebook implements a system for evaluating the results of the LLM generated planned trajectories against the GT.\n",
    "- Its supports both the OpenAI Eval Framework (BLEU score) along with a fine grained custom manual eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before launching this notebook, you need to have executed first the `tau_eval_gen_planned_trajectories.ipynb` and if you wish to use the OpenAI Eval API you need to create `.env` file with your OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, List, Optional, Any, Tuple, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY not set. Please add it to your .env file.\")\n",
    "\n",
    "EVAL_FILE_PATH = \"eval_data.json\"\n",
    "RETAIL_DATA_PATH = \"retail_eval_results.parquet\"\n",
    "AIRLINE_DATA_PATH = \"airline_eval_results.parquet\"\n",
    "if not os.path.exists(RETAIL_DATA_PATH):\n",
    "    raise FileNotFoundError(f\"{RETAIL_DATA_PATH} not found. Please generate it first running `tau_eval_gen_planned_trajectories.ipynb`.\")\n",
    "if not os.path.exists(AIRLINE_DATA_PATH):\n",
    "    raise FileNotFoundError(f\"{AIRLINE_DATA_PATH} not found. Please generate it first running `tau_eval_gen_planned_trajectories.ipynb`.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid evaluation engines: 'manual' or 'openai'\n",
    "# - manual: Custom evaluation metrics\n",
    "# - openai: Uses OpenAI's evaluation framework with BLEU score\n",
    "EVAL_ENGINE = \"openai\"\n",
    "if EVAL_ENGINE not in [\"manual\", \"openai\"]:\n",
    "    raise ValueError(\"EVAL_ENGINE must be either 'manual' or 'openai'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OpenAI Eval API\n",
    "def create_eval():\n",
    "    url = \"https://api.openai.com/v1/evals\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"name\": \"Planned Trajectory\",\n",
    "        \"data_source_config\": {\n",
    "            \"type\": \"custom\",\n",
    "            \"item_schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"instructions\": {\"type\": \"string\"},\n",
    "                    \"planned_trajectory\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"name\": {\"type\": \"string\"},\n",
    "                                \"arguments\": {\"type\": \"object\"}\n",
    "                            },\n",
    "                            \"required\": [\"name\", \"arguments\"]\n",
    "                        }\n",
    "                    },\n",
    "                    \"output_planned_trajectory\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"name\": {\"type\": \"string\"},\n",
    "                                \"arguments\": {\"type\": \"object\"}\n",
    "                            },\n",
    "                            \"required\": [\"name\", \"arguments\"]\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"instructions\", \"planned_trajectory\", \"output_planned_trajectory\"]\n",
    "            }\n",
    "        },\n",
    "        \"testing_criteria\": [\n",
    "            {\n",
    "                \"type\": \"text_similarity\", \n",
    "                \"name\": \"Match output to human label\",\n",
    "                \"evaluation_metric\": \"bleu\",\n",
    "                \"input\": \"{{ item.planned_trajectory }}\",\n",
    "                \"pass_threshold\": 0.5,\n",
    "                \"reference\": \"{{ item.output_planned_trajectory }}\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()\n",
    "\n",
    "def upload_eval_file(file_path):\n",
    "    url = \"https://api.openai.com/v1/files\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n",
    "    }\n",
    "    files = {\n",
    "        'file': open(file_path, 'rb'),\n",
    "        'purpose': (None, 'evals')\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, files=files)\n",
    "    return response.json()\n",
    "\n",
    "def create_eval_run(eval_id, file_id):\n",
    "    url = f\"https://api.openai.com/v1/evals/{eval_id}/runs\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"data_source\": {\n",
    "            \"type\": \"jsonl\",\n",
    "            \"source\": {\n",
    "                \"type\": \"file_id\",\n",
    "                \"id\": file_id\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()\n",
    "\n",
    "def get_eval_run(eval_id, run_id):\n",
    "    url = f\"https://api.openai.com/v1/evals/{eval_id}/runs/{run_id}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "def format_eval_data(retail_df, airline_df, output_file):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for _, row in retail_df.iterrows():\n",
    "            json_obj = {\n",
    "                \"item\": {\n",
    "                    \"instructions\": row[\"instruction\"],\n",
    "                    \"planned_trajectory\": row[\"ground_truth_calls\"],\n",
    "                    \"output_planned_trajectory\": row[\"retail_planned_calls\"]\n",
    "                }\n",
    "            }\n",
    "            f.write(json.dumps(json_obj) + \"\\n\")\n",
    "            \n",
    "        for _, row in airline_df.iterrows():\n",
    "            json_obj = {\n",
    "                \"item\": {\n",
    "                    \"instructions\": row[\"instruction\"],\n",
    "                    \"planned_trajectory\": row[\"ground_truth_calls\"],\n",
    "                    \"output_planned_trajectory\": row[\"airline_planned_calls\"]\n",
    "                }\n",
    "            }\n",
    "            f.write(json.dumps(json_obj) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manual Eval\n",
    "def canonical_params(arguments: Any) -> Tuple:\n",
    "    if isinstance(arguments, dict):\n",
    "        return tuple(sorted((k, canonical_params(v)) for k, v in arguments.items()))\n",
    "    elif isinstance(arguments, list):\n",
    "        return tuple(canonical_params(item) for item in arguments)\n",
    "    return arguments\n",
    "\n",
    "def compare_function_calls(gt_calls: List[Dict[str, Any]],planned_calls: List[Dict[str, Any]],mode: str = 'strict_no_parameters') -> bool:\n",
    "    if not isinstance(planned_calls, list):\n",
    "        return isinstance(gt_calls, list) and not gt_calls if not planned_calls else False\n",
    "    if not isinstance(gt_calls, list):\n",
    "        return not planned_calls # Only match if planned_calls is also empty\n",
    "\n",
    "    valid_planned_calls = [\n",
    "        call for call in planned_calls\n",
    "        if isinstance(call, dict) and 'name' in call and 'error' not in call\n",
    "    ]\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Ground Truth\n",
    "        gt_names = [call['name'] for call in gt_calls if isinstance(call, dict) and 'name' in call]\n",
    "        gt_params_canonical = [canonical_params(call.get('arguments', {})) for call in gt_calls if isinstance(call, dict)]\n",
    "        gt_tuples = list(zip(gt_names, gt_params_canonical))\n",
    "\n",
    "        # Planned\n",
    "        planned_names = [call['name'] for call in valid_planned_calls]\n",
    "        planned_params_canonical = [canonical_params(call.get('arguments', {})) for call in valid_planned_calls]\n",
    "        planned_tuples = list(zip(planned_names, planned_params_canonical))\n",
    "        \n",
    "\n",
    "    except KeyError as e:\n",
    "        logging.warning(f\"Warning: Missing expected key ({e}) in call structure. Assuming mismatch.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Warning: Error extracting call information: {e}. Assuming mismatch.\")\n",
    "        return False\n",
    "    \n",
    "    if mode == 'strict_no_parameters':\n",
    "        return gt_names == planned_names\n",
    "\n",
    "    elif mode == 'strict':\n",
    "        return gt_tuples == planned_tuples\n",
    "\n",
    "    elif mode == 'partial_no_parameters':\n",
    "\n",
    "        return set(gt_names) == set(planned_names)\n",
    "\n",
    "    elif mode == 'partial':\n",
    "        return Counter(gt_tuples) == Counter(planned_tuples)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid comparison mode: {mode}. Choose from 'strict', 'strict_no_parameters', 'partial', 'partial_no_parameters'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 13:17:31,749 - INFO - {'object': 'eval', 'id': 'eval_67f8fa45cb8881908d23e770b00e60f8', 'data_source_config': {'type': 'custom', 'schema': {'type': 'object', 'properties': {'item': {'type': 'object', 'properties': {'instructions': {'type': 'string'}, 'planned_trajectory': {'type': 'array', 'items': {'type': 'object', 'properties': {'name': {'type': 'string'}, 'arguments': {'type': 'object'}}, 'required': ['name', 'arguments']}}, 'output_planned_trajectory': {'type': 'array', 'items': {'type': 'object', 'properties': {'name': {'type': 'string'}, 'arguments': {'type': 'object'}}, 'required': ['name', 'arguments']}}}, 'required': ['instructions', 'planned_trajectory', 'output_planned_trajectory']}}, 'required': ['item']}}, 'testing_criteria': [{'name': 'Match output to human label', 'id': 'Match output to human label-d38ff888-b4d1-4b17-a43c-0c24dc1f2b7c', 'type': 'text_similarity', 'input': '{{ item.planned_trajectory }}', 'reference': '{{ item.output_planned_trajectory }}', 'pass_threshold': 0.5, 'evaluation_metric': 'bleu'}], 'name': 'Planned Trajectory', 'created_at': 1744370245, 'metadata': {}}\n",
      "2025-04-11 13:17:32,701 - INFO - {'object': 'file', 'id': 'file-DPNYCFhGfmau7Fvg6L2463', 'purpose': 'evals', 'filename': 'eval_data.json', 'bytes': 296596, 'created_at': 1744370246, 'expires_at': None, 'status': 'processed', 'status_details': None}\n",
      "2025-04-11 13:17:33,782 - INFO - {'object': 'eval.run', 'id': 'evalrun_67f8fa47845481908cc0863a52c1056e', 'eval_id': 'eval_67f8fa45cb8881908d23e770b00e60f8', 'report_url': 'https://platform.openai.com/evaluations/eval_67f8fa45cb8881908d23e770b00e60f8?project_id=proj_cl9TmTbcG1TtYQe91nWjkXZG&run_id=evalrun_67f8fa47845481908cc0863a52c1056e', 'status': 'queued', 'model': None, 'name': None, 'created_at': 1744370247, 'result_counts': {'total': 0, 'errored': 0, 'failed': 0, 'passed': 0}, 'per_model_usage': None, 'per_testing_criteria_results': None, 'data_source': {'type': 'jsonl', 'source': {'type': 'file_id', 'id': 'file-DPNYCFhGfmau7Fvg6L2463'}}, 'error': None, 'metadata': {}}\n",
      "2025-04-11 13:17:35,194 - INFO - Waiting for evaluation to complete...\n",
      "2025-04-11 13:17:36,609 - INFO - Waiting for evaluation to complete...\n",
      "2025-04-11 13:17:38,054 - INFO - Waiting for evaluation to complete...\n",
      "2025-04-11 13:17:39,475 - INFO - Waiting for evaluation to complete...\n",
      "2025-04-11 13:17:40,893 - INFO - Waiting for evaluation to complete...\n",
      "2025-04-11 13:17:42,330 - INFO - Waiting for evaluation to complete...\n",
      "2025-04-11 13:17:43,814 - INFO - Waiting for evaluation to complete...\n",
      "2025-04-11 13:17:44,751 - INFO - {'object': 'eval.run', 'id': 'evalrun_67f8fa47845481908cc0863a52c1056e', 'eval_id': 'eval_67f8fa45cb8881908d23e770b00e60f8', 'report_url': 'https://platform.openai.com/evaluations/eval_67f8fa45cb8881908d23e770b00e60f8?project_id=proj_cl9TmTbcG1TtYQe91nWjkXZG&run_id=evalrun_67f8fa47845481908cc0863a52c1056e', 'status': 'completed', 'model': None, 'name': None, 'created_at': 1744370247, 'result_counts': {'total': 165, 'errored': 0, 'failed': 124, 'passed': 41}, 'per_model_usage': [], 'per_testing_criteria_results': [{'testing_criteria': 'Match output to human label-d38ff888-b4d1-4b17-a43c-0c24dc1f2b7c', 'passed': 41, 'failed': 124}], 'data_source': {'type': 'jsonl', 'source': {'type': 'file_id', 'id': 'file-DPNYCFhGfmau7Fvg6L2463'}}, 'error': None, 'metadata': {}}\n"
     ]
    }
   ],
   "source": [
    "retail_df = pd.read_parquet(RETAIL_DATA_PATH)\n",
    "airline_df = pd.read_parquet(AIRLINE_DATA_PATH)\n",
    "\n",
    "if EVAL_ENGINE == \"openai\":\n",
    "    format_eval_data(retail_df, airline_df, EVAL_FILE_PATH)\n",
    "\n",
    "    eval = create_eval()\n",
    "    logging.info(eval)\n",
    "    eval_id = eval[\"id\"]\n",
    "\n",
    "    eval_file = upload_eval_file(EVAL_FILE_PATH)\n",
    "    logging.info(eval_file)\n",
    "    eval_file_id = eval_file[\"id\"]\n",
    "    run = create_eval_run(eval_id, eval_file_id)\n",
    "    logging.info(run)\n",
    "    run_id = run[\"id\"]\n",
    "\n",
    "    while get_eval_run(eval_id, run_id)[\"status\"] != \"completed\":\n",
    "        time.sleep(1)\n",
    "        logging.info(\"Waiting for evaluation to complete...\")\n",
    "\n",
    "    result = get_eval_run(eval_id, run_id)\n",
    "    logging.info(result)\n",
    "    \n",
    "elif EVAL_ENGINE == \"manual\":\n",
    "    modes_to_test = ['strict', 'strict_no_parameters', 'partial', 'partial_no_parameters']\n",
    "    logging.info(\"Applying comparison functions...\")\n",
    "    for mode in modes_to_test:\n",
    "        column_name = f'plan_match_{mode}'\n",
    "        logging.info(f\"Calculating for mode: {mode}\")\n",
    "        retail_df[column_name] = retail_df.apply(\n",
    "            lambda row: compare_function_calls(\n",
    "                row.get('ground_truth_calls', []), \n",
    "                row.get('retail_planned_calls', []), \n",
    "                mode=mode\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        airline_df[column_name] = airline_df.apply(\n",
    "            lambda row: compare_function_calls(\n",
    "                row.get('ground_truth_calls', []), \n",
    "                row.get('airline_planned_calls', []), \n",
    "                mode=mode\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    results = {}\n",
    "    for mode in modes_to_test:\n",
    "        column_name = f'plan_match_{mode}'\n",
    "        \n",
    "        if column_name in retail_df.columns:\n",
    "            if retail_df[column_name].dtype == 'bool':\n",
    "                results[f'retail_{mode}'] = retail_df[column_name].mean()\n",
    "            else:\n",
    "                logging.warning(f\"Column '{column_name}' in retail_df does not contain boolean values. Cannot calculate percentage.\")\n",
    "                logging.info(retail_df[column_name].value_counts())\n",
    "        \n",
    "        if column_name in airline_df.columns:\n",
    "            if airline_df[column_name].dtype == 'bool':\n",
    "                results[f'airline_{mode}'] = airline_df[column_name].mean()\n",
    "            else:\n",
    "                logging.warning(f\"Column '{column_name}' in airline_df does not contain boolean values. Cannot calculate percentage.\")\n",
    "                logging.info(airline_df[column_name].value_counts())\n",
    "        results_df = pd.DataFrame([results])\n",
    "        results_df.to_csv(f'evaluation_results_manual.csv', index=False)\n",
    "else:\n",
    "    raise ValueError(\"Invalid EVAL_ENGINE. Please set EVAL_ENGINE to 'openai' or 'manual'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
